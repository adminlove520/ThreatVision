import json
import os
import datetime
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import Config
from .logger import setup_logger
from .blog_manager import BlogManager
from .dingtalk import DingTalkSender

logger = setup_logger(__name__)

class ArticleManager:
    def __init__(self):
        self.processed_urls_file = os.path.join(Config.DATA_DIR, 'processed_urls.json')
        self.processed_urls = self.load_processed_urls()
        self.blog_manager = BlogManager()
        self.dingtalk_sender = DingTalkSender()
        
        # Category emoji mapping
        self.category_emoji = {
            "æ¼æ´åˆ†æ": "ğŸ”",
            "å®‰å…¨ç ”ç©¶": "ğŸ”¬",
            "å¨èƒæƒ…æŠ¥": "ğŸ¯",
            "å®‰å…¨å·¥å…·": "ğŸ› ï¸",
            "æœ€ä½³å®è·µ": "ğŸ“š",
            "åƒç“œæ–°é—»": "ğŸ‰",
            "å…¶ä»–": "ğŸ“Œ"
        }

    def load_processed_urls(self):
        if os.path.exists(self.processed_urls_file):
            try:
                with open(self.processed_urls_file, 'r', encoding='utf-8') as f:
                    return set(json.load(f))
            except Exception as e:
                logger.error(f"Error loading processed URLs: {e}")
                return set()
        return set()

    def save_processed_urls(self):
        try:
            with open(self.processed_urls_file, 'w', encoding='utf-8') as f:
                json.dump(list(self.processed_urls), f, ensure_ascii=False, indent=4)
        except Exception as e:
            logger.error(f"Error saving processed URLs: {e}")

    def is_new_url(self, url):
        return url not in self.processed_urls

    def mark_as_processed(self, url):
        self.processed_urls.add(url)
        self.save_processed_urls()

    def classify_articles(self, articles, analyzer):
        """Classify articles using AI"""
        classified = {}
        for article in articles:
            try:
                category = analyzer.classify_article(article.get('title', ''), article.get('source', ''))
                if category not in classified:
                    classified[category] = []
                classified[category].append(article)
                logger.info(f"Classified '{article['title'][:50]}...' as {category}")
            except Exception as e:
                logger.error(f"Error classifying article: {e}")
                if "å…¶ä»–" not in classified:
                    classified["å…¶ä»–"] = []
                classified["å…¶ä»–"].append(article)
        return classified

    def format_cve_section(self, cve):
        """Format CVE with detailed analysis"""
        content = f"\n### {cve.cve_id}"
        
        # Try to extract repo name if available
        if hasattr(cve, 'url') and cve.url:
            content += f" - {cve.url.split('/')[-1]}\n\n"
        else:
            content += "\n\n"
        
        content += "#### ğŸ“Œ æ¼æ´ä¿¡æ¯\n\n"
        content += "| å±æ€§ | è¯¦æƒ… |\n"
        content += "|------|------|\n"
        content += f"| CVEç¼–å· | {cve.cve_id} |\n"
        
        if cve.ai_analysis:
            try:
                analysis = json.loads(cve.ai_analysis)
                
                risk_level = analysis.get('risk_level', 'MEDIUM')
                # Map Chinese risk levels back to English keywords if necessary
                if 'é«˜' in risk_level: risk_level = 'HIGH'
                elif 'ä¸­' in risk_level: risk_level = 'MEDIUM'
                elif 'ä½' in risk_level: risk_level = 'LOW'
                elif 'ä¸¥é‡' in risk_level: risk_level = 'CRITICAL'
                content += f"| é£é™©ç­‰çº§ | `{risk_level}` |\n"
                content += f"| åˆ©ç”¨çŠ¶æ€ | `{analysis.get('exploitation_status', 'æœªçŸ¥')}` |\n"
                
                if hasattr(cve, 'publish_date'):
                    content += f"| å‘å¸ƒæ—¶é—´ | {cve.publish_date} |\n"
                
                content += "\n#### ğŸ’¡ åˆ†ææ¦‚è¿°\n\n"
                content += f"{analysis.get('summary', 'N/A')}\n\n"
                
                if 'key_findings' in analysis and analysis['key_findings']:
                    content += "#### ğŸ” å…³é”®å‘ç°\n\n"
                    content += "| åºå· | å‘ç°å†…å®¹ |\n"
                    content += "|------|----------|\n"
                    for idx, finding in enumerate(analysis['key_findings'], 1):
                        content += f"| {idx} | {finding} |\n"
                    content += "\n"
                
                if 'technical_details' in analysis and analysis['technical_details']:
                    content += "#### ğŸ› ï¸ æŠ€æœ¯ç»†èŠ‚\n\n"
                    for detail in analysis['technical_details']:
                        content += f"> {detail}\n\n"
                    content += "\n"
                
                if 'affected_components' in analysis and analysis['affected_components']:
                    content += "#### ğŸ¯ å—å½±å“ç»„ä»¶\n\n"
                    content += "```\n"
                    for comp in analysis['affected_components']:
                        content += f"â€¢ {comp}\n"
                    content += "```\n\n"
                
                if 'value_assessment' in analysis:
                    content += "#### âš¡ ä»·å€¼è¯„ä¼°\n\n"
                    content += "<details>\n"
                    content += "<summary>å±•å¼€æŸ¥çœ‹è¯¦ç»†è¯„ä¼°</summary>\n\n"
                    content += f"{analysis['value_assessment']}\n"
                    content += "</details>\n\n"
                
            except Exception as e:
                logger.error(f"Error formatting CVE {cve.cve_id}: {e}")
                content += f"\n**æè¿°**: {cve.description}\n\n"
        else:
            content += f"\n**æè¿°**: {cve.description}\n"
            content += f"\n**CVSSè¯„åˆ†**: {cve.cvss_score}\n\n"
        
        content += "---\n"
        return content

    def format_repo_section(self, repo):
        """Format repository with detailed analysis"""
        content = f"\n### {repo.name}"
        
        if hasattr(repo, 'url') and repo.url:
            content += f" - [{repo.name}]({repo.url})\n\n"
        else:
            content += "\n\n"
        
        content += "#### ğŸ“Œ ä»“åº“ä¿¡æ¯\n\n"
        content += "| å±æ€§ | è¯¦æƒ… |\n"
        content += "|------|------|\n"
        content += f"| ä»“åº“åç§° | [{repo.name}]({repo.url}) |\n"
        
        if repo.ai_analysis:
            try:
                analysis = json.loads(repo.ai_analysis)
                
                content += f"| é£é™©ç­‰çº§ | `{analysis.get('risk_level', 'MEDIUM')}` |\n"
                content += f"| å®‰å…¨ç±»å‹ | `{analysis.get('security_type', 'å…¶ä»–')}` |\n"
                content += f"| æ›´æ–°ç±»å‹ | `{analysis.get('update_type', 'GENERAL_UPDATE')}` |\n"
                content += "\n"
                
                if repo.stars:
                    content += f"#### ğŸ“Š ä»£ç ç»Ÿè®¡\n\n"
                    content += f"- â­ Stars: **{repo.stars}**\n\n"
                
                content += "#### ğŸ’¡ åˆ†ææ¦‚è¿°\n\n"
                content += f"{analysis.get('summary', repo.description)}\n\n"
                
                if 'key_findings' in analysis and analysis['key_findings']:
                    content += "#### ğŸ” å…³é”®å‘ç°\n\n"
                    content += "| åºå· | å‘ç°å†…å®¹ |\n"
                    content += "|------|----------|\n"
                    for idx, finding in enumerate(analysis['key_findings'], 1):
                        content += f"| {idx} | {finding} |\n"
                    content += "\n"
                
                if 'technical_details' in analysis and analysis['technical_details']:
                    content += "#### ğŸ› ï¸ æŠ€æœ¯ç»†èŠ‚\n\n"
                    for detail in analysis['technical_details']:
                        content += f"> {detail}\n\n"
                    content += "\n"
                
                if 'affected_components' in analysis and analysis['affected_components']:
                    content += "#### ğŸ¯ å—å½±å“ç»„ä»¶\n\n"
                    content += "```\n"
                    for comp in analysis['affected_components']:
                        content += f"â€¢ {comp}\n"
                    content += "```\n\n"
                
                if 'value_assessment' in analysis:
                    content += "#### âš¡ ä»·å€¼è¯„ä¼°\n\n"
                    content += "<details>\n"
                    content += "<summary>å±•å¼€æŸ¥çœ‹è¯¦ç»†è¯„ä¼°</summary>\n\n"
                    content += f"{analysis['value_assessment']}\n"
                    content += "</details>\n\n"
                
            except Exception as e:
                logger.error(f"Error formatting repo {repo.name}: {e}")
                content += f"\n**æè¿°**: {repo.description}\n\n"
        else:
            content += f"\n**æè¿°**: {repo.description}\n"
            content += f"\n**Stars**: {repo.stars}\n\n"
        
        content += "---\n"
        return content

    def generate_daily_report(self, cve_data, repo_data, articles_data, analyzer=None):
        """
        Generate a professional markdown report
        """
        now = datetime.datetime.now()
        date_str = now.strftime('%Y-%m-%d')
        year_str = now.strftime('%Y')
        time_str = now.strftime('%Y-%m-%d %H:%M:%S')
        
        title = f"å®‰å…¨èµ„è®¯æ—¥æŠ¥ {date_str}"
        
        # Header
        content = f"\n# {title}\n\n"
        content += "> æœ¬æ–‡ç”±AIè‡ªåŠ¨ç”Ÿæˆ,åŸºäºå¯¹å®‰å…¨ç›¸å…³ä»“åº“ã€CVEå’Œæœ€æ–°å®‰å…¨ç ”ç©¶æˆæœçš„è‡ªåŠ¨åŒ–åˆ†æã€‚\n"
        content += f"> \n"
        content += f"> æ›´æ–°æ—¶é—´:{time_str}\n\n"
        content += "<!-- more -->\n\n"
        
        # Today's News Section
        content += "## ä»Šæ—¥èµ„è®¯\n\n"
        
        if articles_data and analyzer:
            classified = self.classify_articles(articles_data, analyzer)
            
            # Order categories
            category_order = ["æ¼æ´åˆ†æ", "å®‰å…¨ç ”ç©¶", "å¨èƒæƒ…æŠ¥", "å®‰å…¨å·¥å…·", "æœ€ä½³å®è·µ", "åƒç“œæ–°é—»", "å…¶ä»–"]
            
            for category in category_order:
                if category in classified and classified[category]:
                    emoji = self.category_emoji.get(category, "ğŸ“Œ")
                    content += f"### {emoji} {category}\n\n"
                    for article in classified[category]:
                        content += f"* [{article['title']}]({article['url']})\n"
                    content += "\n"
        elif articles_data:
            # Fallback: no classification
            content += "### ğŸ“° å®‰å…¨æ–‡ç« \n\n"
            for article in articles_data:
                content += f"* [{article['title']}]({article['url']})\n"
            content += "\n"
        else:
            content += "ä»Šæ—¥æš‚æ— æ–°æ–‡ç« ã€‚\n\n"
        
        # Security Analysis Section
        content += "## å®‰å…¨åˆ†æ\n"
        content += f"({date_str})\n\n"
        content += "æœ¬æ–‡æ¡£åŒ…å« AI å¯¹å®‰å…¨ç›¸å…³å†…å®¹çš„è‡ªåŠ¨åŒ–åˆ†æç»“æœã€‚\n\n"
        
        # CVE Analysis
        if cve_data:
            for cve in cve_data:
                content += self.format_cve_section(cve)
        
        # Repository Analysis
        if repo_data:
            for repo in repo_data:
                content += self.format_repo_section(repo)
        
        if not cve_data and not repo_data:
            content += "ä»Šæ—¥æš‚æ— é‡è¦å®‰å…¨åˆ†æå†…å®¹ã€‚\n\n"
        
        # Save locally: /YYYY/YYYY-MM-DD/Daily_YYYY-MM-DD.md
        report_dir = os.path.join(Config.DATA_DIR, year_str, date_str)
        os.makedirs(report_dir, exist_ok=True)
        report_path = os.path.join(report_dir, f"Daily_{date_str}.md")
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        logger.info(f"Generated daily report: {report_path}")

        # Publish to blog if enabled
        if Config.ENABLE_BLOG_PUBLISH:
            self.blog_manager.publish_article(title, content)
        else:
            logger.info("Blog publishing is disabled.")

        # Push to DingTalk
        summary = f"# {title}\n\nç”Ÿæˆäº†{len(cve_data) if cve_data else 0}ä¸ªCVEåˆ†æå’Œ{len(repo_data) if repo_data else 0}ä¸ªä»“åº“åˆ†æã€‚"
        self.dingtalk_sender.send_markdown(title, summary)
        
        # Update RSS feed
        try:
            from utils.rss_generator import RSSGenerator
            rss_generator = RSSGenerator()
            rss_generator.update_rss()
            logger.info("RSS feed updated successfully")
        except Exception as e:
            logger.error(f"Failed to update RSS feed: {e}")
        
        # Push report to GitHub Release
        try:
            from utils.github_release import GitHubReleaseManager
            github_release_manager = GitHubReleaseManager()
            github_release_manager.push_report_to_release(report_path)
            logger.info("Report pushed to GitHub Release successfully")
        except Exception as e:
            logger.error(f"Failed to push report to GitHub Release: {e}")
        
        return report_path
